{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Computer-Aided Diagnosis System for Lung Fibrosis: From the Effect of Radiomic Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Necessary Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "from radiomics import featureextractor\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Paramaeters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 1)\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "data_dir = '/content/drive/MyDrive/FYP/UNET/train'  # Path to your training data\n",
    "model_path = '/content/drive/MyDrive/FYP/UNET/lung_segmentation_unet.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for filename in os.listdir(os.path.join(data_dir, 'images')):\n",
    "        img = cv2.imread(os.path.join(data_dir, 'images', filename), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (128, 128))  # Resize to desired size\n",
    "        images.append(img)\n",
    "\n",
    "        mask = cv2.imread(os.path.join(data_dir, 'masks', filename), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (128, 128))  # Resize to desired size\n",
    "        masks.append(mask)\n",
    "\n",
    "    images = np.array(images).astype('float32') / 255.0  # Normalize\n",
    "    masks = np.array(masks).astype('float32') / 255.0  # Normalize\n",
    "    masks = np.expand_dims(masks, axis=-1)  # Expand dims for channel\n",
    "\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(data_dir):\n",
    "    images, masks = load_data(data_dir)\n",
    "    return train_test_split(images, masks, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_flat * y_pred_flat)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_flat) + np.sum(y_pred_flat) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_flat * y_pred_flat)\n",
    "    return intersection / (np.sum(y_true_flat) + np.sum(y_pred_flat) - intersection + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(y_true, y_pred):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    return np.sum(y_true_flat == y_pred_flat) / y_true_flat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        \"Dice Coefficient\": dice_coefficient(y_true, y_pred),\n",
    "        \"IoU\": iou(y_true, y_pred),\n",
    "        \"Pixel Accuracy\": pixel_accuracy(y_true, y_pred),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Contracting Path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Expanding Path\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming get_data_splits, unet, and other variables like model_path, batch_size, and epochs are defined elsewhere\n",
    "\n",
    "X_train, X_val, y_train, y_val = get_data_splits(data_dir)\n",
    "\n",
    "# Check if the saved model exists\n",
    "if os.path.exists(model_path):\n",
    "    # Load the saved model in TensorFlow's SavedModel format\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Loaded saved model from {model_path}\")\n",
    "else:\n",
    "    # Create a U-Net model and train it\n",
    "    input_shape = (128, 128, 1)  # Define input shape without batch dimension\n",
    "    model = unet(input_shape)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    # Save the trained model in SavedModel format\n",
    "    model.save(model_path)  # No .h5 extension to use SavedModel format\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Save the predictions\n",
    "np.save('/content/drive/MyDrive/FYP/UNET/predictions.npy', predictions)\n",
    "\n",
    "def visualize_predictions(X_val, y_val, predictions):\n",
    "    thresholded_preds = (predictions > 0.5).astype(np.uint8)  # Threshold at 0.5\n",
    "    for i in range(5):  # Display 5 samples\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(X_val[i].squeeze(), cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(y_val[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(thresholded_preds[i].squeeze(), cmap='gray')\n",
    "        plt.title('Prediction')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Visualize the predictions\n",
    "visualize_predictions(X_val, y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, evaluate metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    dice = dice_coefficient(y_true, y_pred)\n",
    "    iou_score = iou(y_true, y_pred)\n",
    "    accuracy = pixel_accuracy(y_true, y_pred)\n",
    "    print(f\"Dice Coefficient: {dice}\")\n",
    "    print(f\"IoU: {iou_score}\")\n",
    "    print(f\"Pixel Accuracy: {accuracy}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "evaluate_metrics(y_val, (predictions > 0.5).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss values over epochs\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "if 'history' in locals():  # Make sure history exists from training\n",
    "    plot_training_history(history)\n",
    "\n",
    "# Visualize the predictions\n",
    "visualize_predictions(X_val, y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set model parameters and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (128, 128, 1)\n",
    "model_path = '/content/drive/MyDrive/FYP/DataSet/lung_segmentation_unet.h5'\n",
    "covid_data_dir = '/content/drive/MyDrive/FYP/segmentation/Covid'  # Path to COVID images folder\n",
    "output_dir = '/content/drive/MyDrive/FYP/Masks'  # Directory to save the predicted masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the pre-trained UNet model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model in .h5 format\n",
    "model_path_h5 = '/content/drive/MyDrive/FYP/DataSet/lung_segmentation_unet.h5'  # Path to your .h5 model\n",
    "saved_model_dir = '/content/drive/MyDrive/FYP/DataSet/saved_model'  # Directory to save the new SavedModel\n",
    "\n",
    "# Load the .h5 model\n",
    "model = load_model(model_path_h5, compile=False)\n",
    "\n",
    "# Save the model in SavedModel format\n",
    "model.save(saved_model_dir)\n",
    "print(f\"Model saved in SavedModel format at {saved_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensure output directory exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loop through each image in the COVID folder, segment, and save the masks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(covid_data_dir):\n",
    "    # Load and preprocess the image\n",
    "    img_path = os.path.join(covid_data_dir, filename)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (128, 128))  # Resize to match model input shape\n",
    "    img_normalized = img_resized.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "    img_input = np.expand_dims(img_normalized, axis=(0, -1))  # Add batch and channel dimensions\n",
    "\n",
    "    # Predict the segmentation mask\n",
    "    predicted_mask = model.predict(img_input)[0, :, :, 0]  # Remove batch dimension\n",
    "\n",
    "    # Threshold the mask to binary (0, 1)\n",
    "    thresholded_mask = (predicted_mask > 0.5).astype(np.uint8) * 255  # Convert to binary mask for visualization/storage\n",
    "\n",
    "    # Resize mask back to original image size if necessary\n",
    "    mask_resized = cv2.resize(thresholded_mask, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Save the mask\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(output_path, mask_resized)\n",
    "    print(f\"Saved mask to {output_path}\")\n",
    "\n",
    "print(\"Mask generation for the COVID dataset is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your images and masks\n",
    "covid_data_dir = '/content/drive/MyDrive/FYP/segmentation/Covid'  # Original images\n",
    "output_dir = '/content/drive/MyDrive/FYP/Masks'  # Generated masks\n",
    "\n",
    "def visualize_predictions(image_dir, mask_dir, num_samples=5):\n",
    "    images = sorted(os.listdir(image_dir))[:num_samples]\n",
    "\n",
    "    for filename in images:\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        mask_path = os.path.join(mask_dir, filename)\n",
    "\n",
    "        # Load images and masks for visualization\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        pred_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Display original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        # Display generated mask\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(pred_mask, cmap='gray')\n",
    "        plt.title('Generated Mask')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(covid_data_dir, output_dir, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyRadiomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the extractor with the YAML configuration file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = featureextractor.RadiomicsFeatureExtractor('/content/drive/MyDrive/FYP/FeatureExtraction/params.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paths to image and mask folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"/content/drive/MyDrive/FYP/segmentation/Covid\"\n",
    "mask_folder = \"/content/drive/MyDrive/FYP/Masks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the output CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = \"/content/drive/MyDrive/FYP/extracted_features.csv\"\n",
    "feature_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get all image and mask files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(os.listdir(image_folder))\n",
    "mask_files = sorted(os.listdir(mask_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loop through each image and corresponding mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_file, mask_file in zip(image_files, mask_files):\n",
    "    image_path = os.path.join(image_folder, img_file)\n",
    "    mask_path = os.path.join(mask_folder, mask_file)\n",
    "\n",
    "    # Read the image and mask\n",
    "    image = sitk.ReadImage(image_path)\n",
    "    mask = sitk.ReadImage(mask_path)\n",
    "\n",
    "    # Convert images to grayscale if necessary\n",
    "    if image.GetNumberOfComponentsPerPixel() > 1:\n",
    "        image = sitk.VectorIndexSelectionCast(image, 0)  # Select one channel, e.g., Red channel\n",
    "\n",
    "    if mask.GetNumberOfComponentsPerPixel() > 1:\n",
    "        mask = sitk.VectorIndexSelectionCast(mask, 0)  # Ensure mask is single-channel as well\n",
    "\n",
    "    # Ensure images are of type UInt8 or UInt16, compatible with radiomics processing\n",
    "    image = sitk.Cast(image, sitk.sitkUInt8)\n",
    "    mask = sitk.Cast(mask, sitk.sitkUInt8)\n",
    "\n",
    "    # Check if the image and mask sizes match\n",
    "    if image.GetSize() != mask.GetSize():\n",
    "        print(f\"Resizing mask for {img_file} to match image dimensions.\")\n",
    "        mask = sitk.Resample(mask, referenceImage=image, transform=sitk.Transform(),\n",
    "                             interpolator=sitk.sitkNearestNeighbor, defaultPixelValue=0, outputPixelType=sitk.sitkUInt8)\n",
    "\n",
    "    # Check if the label 255 exists in the mask\n",
    "    mask_stats = sitk.LabelStatisticsImageFilter()\n",
    "    mask_stats.Execute(image, mask)\n",
    "\n",
    "    if 255 in mask_stats.GetLabels():  # Proceed only if label 255 is present\n",
    "        # Extract only the specified features\n",
    "        features = extractor.execute(image, mask)\n",
    "\n",
    "        # Store features in a dictionary, with image filename as identifier\n",
    "        feature_row = {\"Image\": img_file}\n",
    "        feature_row.update({feature_name: feature_value for feature_name, feature_value in features.items()})\n",
    "\n",
    "    else:\n",
    "        # If label 255 is missing, add an empty row for this image\n",
    "        print(f\"No segmentation found in {img_file}. Adding empty row.\")\n",
    "        feature_row = {\"Image\": img_file}\n",
    "\n",
    "    # Append the row to the data list\n",
    "    feature_data.append(feature_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the list of dictionaries to a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the DataFrame to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Feature extraction complete. Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random-Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the YAML file and construct full feature names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/FYP/Old codes/Feature Extraction/params.yaml', 'r') as file:\n",
    "    params_data = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct feature column names based on the hierarchical structure in params.yaml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [f\"original_{feature_class}_{feature}\" for feature_class, features in params_data['featureClass'].items() for feature in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = pd.read_csv('/content/drive/MyDrive/FYP/extracted_features_normal.csv')\n",
    "covid_df = pd.read_csv('/content/drive/MyDrive/FYP/extracted_features_Covid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign labels and combine datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df['Target'] = 0  # Label for 'Normal' cases\n",
    "covid_df['Target'] = 1   # Label for 'COVID' cases\n",
    "df = pd.concat([normal_df, covid_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing feature columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_features = [col for col in feature_columns if col not in df.columns]\n",
    "if missing_features:\n",
    "    print(\"The following features are missing from the dataset:\", missing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select only the feature columns and the target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_columns]\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fillna(X.mean(), inplace=True)  # Fill missing values with column mean\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize and train Random Forest classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(rf_clf, X_scaled, y, cv=5)\n",
    "print(\"Cross-validation accuracy scores:\", cv_scores)\n",
    "print(\"Mean cross-validation accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions and evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_clf.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_clf.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf_clf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10), palette='viridis')\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap of Feature Correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = pd.DataFrame(X_scaled, columns=feature_columns).corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=False, fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation Scores Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Cross-Validation Scores across Folds\")\n",
    "plt.xlabel(\"Fold\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
